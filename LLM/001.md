## Transformer

为什么都用 pre-norm 而不用 post-norm

行存储, view 不同但存储相同, 注意内存移动的消耗(not only FLOPS matters)

(raw) location embedding

多头插入维数的位置, 要 token 之间交互

Casual Masking: 把右上三角掩码(设为负无穷), 不能看到未来的 token

QKV 为什么要除以根号(算方差的时候要平方, 归一化)
